# ðŸ“š EduRAG â€“ AI Study Assistant
### A production-ready Retrieval-Augmented Generation (RAG) system for students

---

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        STUDENT BROWSER                          â”‚
â”‚                    (Streamlit Frontend)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ PDF upload / Question
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RAG BACKEND  (rag_backend.py)                â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ PDF          â”‚   â”‚  Text         â”‚   â”‚  Embedding       â”‚   â”‚
â”‚  â”‚ Extractor    â”‚â”€â”€â–¶â”‚  Chunker      â”‚â”€â”€â–¶â”‚  Model           â”‚   â”‚
â”‚  â”‚ (PyMuPDF)    â”‚   â”‚  (overlap)    â”‚   â”‚  (MiniLM-L6)     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                   â”‚ vectors     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                  FAISS Vector Store                       â”‚  â”‚
â”‚  â”‚   (inner-product search + MMR diversity re-ranking)       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                   â”‚ top-k chunksâ”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚               LLM Generator                              â”‚  â”‚
â”‚  â”‚   GPT-4o-mini  OR  Claude Sonnet  OR  Extractive fallbackâ”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                   â”‚ answer      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                    â–¼
                                        Student sees the answer
```

---

## Quick Start

### 1. Install dependencies
```bash
cd rag_app
pip install -r requirements.txt
```

### 2. Set your LLM API key (pick one)
```bash
# Option A: OpenAI
export OPENAI_API_KEY="sk-..."

# Option B: Anthropic Claude
export ANTHROPIC_API_KEY="sk-ant-..."

# Or create a .env file:
echo "OPENAI_API_KEY=sk-..." > .env
```

### 3. Run the app
```bash
streamlit run app.py
```

Open http://localhost:8501 in your browser.

---

## How It Works â€” Step by Step

### Step 1 Â· PDF Ingestion Pipeline

```
PDF file
  â””â”€â–¶ PyMuPDF extracts text block-by-block, preserving font sizes
        â””â”€â–¶ Large / bold text â†’ detected as section headings
              â””â”€â–¶ TextChunker splits into ~400-word chunks with 80-word overlap
                    â””â”€â–¶ SentenceTransformer encodes each chunk â†’ 384-dim vector
                          â””â”€â–¶ FAISS IndexFlatIP stores all vectors
```

**Why overlap?** The last 80 words of each chunk are repeated at the start of the
next. This prevents answers being split across chunk boundaries.

**Why section detection?** Headings are stored as metadata so the LLM context
includes `[Chapter 3: Photosynthesis | Page 12]` â€” making answers more precise.

---

### Step 2 Â· Retrieval

```
Student question
  â””â”€â–¶ Embedded to same 384-dim space as chunks
        â””â”€â–¶ FAISS dot-product search â†’ top-k candidates (e.g. k=4)
              â””â”€â–¶ MMR re-ranking â†’ removes near-duplicate chunks
                    â””â”€â–¶ 4 diverse, relevant chunks returned
```

**MMR (Maximal Marginal Relevance)** balances relevance vs. diversity:
`score = Î» Ã— relevance âˆ’ (1âˆ’Î») Ã— similarity_to_already_selected`

This means if two chunks say the same thing, only the better one is kept.

---

### Step 3 Â· Generation

The LLM receives a carefully engineered prompt:

```
SYSTEM: You are EduRAG, a helpful AI tutor for school studentsâ€¦

DOCUMENT EXCERPTS:
[Biology_Textbook.pdf | Page 12 | Photosynthesis]
Photosynthesis is the process by which plantsâ€¦

[Biology_Textbook.pdf | Page 14 | Light Reactions]
The light-dependent reactions occur in the thylakoidâ€¦

STUDENT QUESTION:
What happens during the light reactions of photosynthesis?
```

The LLM is instructed to answer **only from the excerpts**, preventing hallucination.

---

### Step 4 Â· Multi-Document & Multi-Student Scalability

- **Multiple PDFs**: All chunks go into a single FAISS index. Each chunk carries
  its source filename, so answers can cite multiple documents.
- **Multiple students**: Each Streamlit session gets its own Python process
  (Streamlit's default behaviour). For true multi-tenant isolation, deploy
  with Kubernetes and one pod per student session (see Docker section below).

---

## File Structure

```
rag_app/
â”œâ”€â”€ app.py              # Streamlit frontend
â”œâ”€â”€ rag_backend.py      # Full RAG pipeline (extract â†’ chunk â†’ embed â†’ retrieve â†’ generate)
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ Dockerfile          # Container build
â”œâ”€â”€ docker-compose.yml  # Multi-service deployment
â””â”€â”€ README.md           # This file
```

---

## Configuration

| Parameter      | Default | Where to change            | Effect                                  |
|---------------|---------|----------------------------|-----------------------------------------|
| `chunk_size`   | 400 words | `TextChunker(chunk_size=)` | Larger = more context, slower embedding |
| `overlap`      | 80 words  | `TextChunker(overlap=)`    | More overlap = fewer boundary cuts      |
| `top_k`        | 4         | Streamlit sidebar slider   | More chunks = richer context            |
| `MODEL_NAME`   | `all-MiniLM-L6-v2` | `EmbeddingModel` | Swap for a larger model for better quality |
| LLM model      | `gpt-4o-mini` | `LLMGenerator._call_openai` | Use `gpt-4o` for best quality          |

---

## Docker Deployment (Multi-Student)

```bash
# Build and run
docker-compose up --build

# Scale to 4 instances behind a load balancer
docker-compose up --scale edurag=4
```

For production, add **nginx** as a reverse proxy and **Redis** to share the
FAISS index across instances.

---

## Improving the Model Over Time

1. **Log interactions**: Save (question, retrieved_chunks, answer) to a database.
2. **Identify failures**: Mark answers where students clicked "Not helpful".
3. **Fine-tune embeddings**: Use those logs to fine-tune the sentence-transformer
   with contrastive learning (positive = good retrieval, negative = bad).
4. **Prompt iteration**: Refine the system prompt based on answer quality.
5. **Hybrid search**: Add BM25 keyword search alongside dense vectors and
   combine scores with RRF (Reciprocal Rank Fusion) for harder factual queries.

---

## Embedding Model Alternatives

| Model                          | Size   | Quality  | Speed  |
|-------------------------------|--------|----------|--------|
| `all-MiniLM-L6-v2` (default)  | 80 MB  | Good     | Fast   |
| `all-mpnet-base-v2`           | 420 MB | Better   | Medium |
| `text-embedding-3-small` (OpenAI API) | Cloud | Best  | API call |
| `nomic-embed-text` (Ollama)   | Local  | Very good | Medium |

---

## FAQ

**Q: The app works without an API key?**  
A: Yes â€” it uses an extractive fallback that shows the most relevant text chunks directly. Set an API key for full AI-generated, synthesised answers.

**Q: How many pages can it handle?**  
A: Tested up to 500-page textbooks. For larger PDFs, increase `chunk_size` to reduce the number of chunks, or use a persistent vector DB like Chroma or Pinecone.

**Q: Can I use a local LLM (no API cost)?**  
A: Yes â€” replace `_call_openai` in `LLMGenerator` with an Ollama call:
```python
import ollama
resp = ollama.chat(model="llama3", messages=[...])
```
